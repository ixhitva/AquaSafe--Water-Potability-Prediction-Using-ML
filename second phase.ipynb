{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2400bdce-8a70-4cf6-831d-6f8f714acd18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2785 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          2495 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3114 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n",
      "None\n",
      "\n",
      "Dataset Description:\n",
      "                ph     Hardness        Solids  Chloramines      Sulfate  \\\n",
      "count  2785.000000  3276.000000   3276.000000  3276.000000  2495.000000   \n",
      "mean      7.080795   196.369496  22014.092526     7.122277   333.775777   \n",
      "std       1.594320    32.879761   8768.570828     1.583085    41.416840   \n",
      "min       0.000000    47.432000    320.942611     0.352000   129.000000   \n",
      "25%       6.093092   176.850538  15666.690297     6.127421   307.699498   \n",
      "50%       7.036752   196.967627  20927.833607     7.130299   333.073546   \n",
      "75%       8.062066   216.667456  27332.762127     8.114887   359.950170   \n",
      "max      14.000000   323.124000  61227.196008    13.127000   481.030642   \n",
      "\n",
      "       Conductivity  Organic_carbon  Trihalomethanes    Turbidity   Potability  \n",
      "count   3276.000000     3276.000000      3114.000000  3276.000000  3276.000000  \n",
      "mean     426.205111       14.284970        66.396293     3.966786     0.390110  \n",
      "std       80.824064        3.308162        16.175008     0.780382     0.487849  \n",
      "min      181.483754        2.200000         0.738000     1.450000     0.000000  \n",
      "25%      365.734414       12.065801        55.844536     3.439711     0.000000  \n",
      "50%      421.884968       14.218338        66.622485     3.955028     0.000000  \n",
      "75%      481.792304       16.557652        77.337473     4.500320     1.000000  \n",
      "max      753.342620       28.300000       124.000000     6.739000     1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('water_potability.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDataset Description:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a8b76b-4a3c-40f0-8d79-5cb4a941a895",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 2: Exploratory Data Analysis (EDA)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Ensure all plots are saved correctly\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Missing Values Heatmap\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(df\u001b[38;5;241m.\u001b[39misnull(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m, cbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, yticklabels\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing Values Heatmap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 2: Exploratory Data Analysis (EDA)\n",
    "# Ensure all plots are saved correctly\n",
    "\n",
    "# Missing Values Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=df.index)\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.savefig('plots/missing_values_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Class Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Potability', data=df)\n",
    "plt.title('Class Distribution')\n",
    "plt.savefig('plots/class_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.savefig('plots/correlation_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Pair Plot of Key Features\n",
    "key_features = ['ph', 'Hardness', 'Chloramines', 'Sulfate', 'Potability']\n",
    "sns.pairplot(df[key_features], hue='Potability', diag_kind='hist')\n",
    "plt.savefig('plots/pair_plot_key_features.png')\n",
    "plt.close()\n",
    "\n",
    "# Feature Distributions (Interactive with Plotly)\n",
    "features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n",
    "for feature in features:\n",
    "    fig = px.histogram(df, x=feature, color='Potability', marginal='box', title=f'{feature} Distribution by Potability')\n",
    "    fig.write_html(f'plots/{feature.lower()}_distribution.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430fb9fb-4a08-457b-be97-b930bc1cecbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Feature engineering\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mph_hardness\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mph\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHardness\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Fixed 'pH' to 'ph'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchloramines_sulfate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChloramines\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSulfate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define plot directory\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "df['ph_hardness'] = df['ph'] * df['Hardness']  # Fixed 'pH' to 'ph'\n",
    "df['chloramines_sulfate'] = df['Chloramines'] * df['Sulfate']\n",
    "\n",
    "# Define plot directory\n",
    "PLOT_DIR = r\"C:\\Users\\iasis\\Ultimate_Water_Potability_Prediction\\plots\"\n",
    "\n",
    "# Graph 14: ph*Hardness Distribution (Previously Graph 5 in the snippet)\n",
    "print(\"\\n### Graph 14: ph*Hardness Distribution ###\")\n",
    "print(\"This histogram visualizes the engineered feature 'ph*Hardness', colored by 'Potability'. The interaction between pH and Hardness may capture combined effects on water potability, potentially improving model performance. We observe how this new feature distributes across potable (1) and non-potable (0) water, which can indicate its predictive power.\")\n",
    "fig = px.histogram(df, x='ph_hardness', color='Potability', title='ph*Hardness Distribution by Potability', \n",
    "                   nbins=30, opacity=0.7)\n",
    "fig.update_layout(\n",
    "    xaxis_title='ph*Hardness', \n",
    "    yaxis_title='Count',\n",
    "    title_font_size=14,\n",
    "    xaxis_title_font_size=12,\n",
    "    yaxis_title_font_size=12,\n",
    "    legend_title='Potability',\n",
    "    legend_title_font_size=12,\n",
    "    legend_font_size=10,\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray'),\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray'),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "fig.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "try:\n",
    "    fig.write_html(os.path.join(PLOT_DIR, 'ph_hardness_distribution.html'))\n",
    "    print(\"Graph 14 saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Graph 14: {e}\")\n",
    "fig.show()\n",
    "\n",
    "# Graph 15: Chloramines*Sulfate Distribution (Previously Graph 5 in the snippet)\n",
    "print(\"\\n### Graph 15: Chloramines*Sulfate Distribution ###\")\n",
    "print(\"This histogram shows the distribution of the engineered feature 'Chloramines*Sulfate', colored by 'Potability'. The interaction between these chemical properties might reveal patterns affecting water safety, enhancing the dataset's predictive power. Differences in distribution between potable and non-potable water suggest this feature may be useful for classification.\")\n",
    "fig = px.histogram(df, x='chloramines_sulfate', color='Potability', title='Chloramines*Sulfate Distribution by Potability', \n",
    "                   nbins=30, opacity=0.7)\n",
    "fig.update_layout(\n",
    "    xaxis_title='Chloramines*Sulfate', \n",
    "    yaxis_title='Count',\n",
    "    title_font_size=14,\n",
    "    xaxis_title_font_size=12,\n",
    "    yaxis_title_font_size=12,\n",
    "    legend_title='Potability',\n",
    "    legend_title_font_size=12,\n",
    "    legend_font_size=10,\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray'),\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray'),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "fig.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "try:\n",
    "    fig.write_html(os.path.join(PLOT_DIR, 'chloramines_sulfate_distribution.html'))\n",
    "    print(\"Graph 15 saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Graph 15: {e}\")\n",
    "fig.show()\n",
    "\n",
    "# KNN imputation\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_knn = pd.DataFrame(imputer_knn.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Skip GAN imputation (using KNN as fallback)\n",
    "df_gan = df_knn  # Fallback to KNN imputation\n",
    "\n",
    "# Graph 16: Missing Values After Imputation (Previously Graph 6 in the snippet)\n",
    "print(\"\\n### Graph 16: Missing Values After Imputation ###\")\n",
    "print(\"These heatmaps compare missing values after KNN imputation and the fallback (also KNN in this case). Since all missing values are imputed, we expect no yellow (missing) regions in either heatmap, confirming that the imputation step was successful.\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.heatmap(df_knn.isnull(), cbar=False, cmap='viridis', ax=axes[0])\n",
    "axes[0].set_title('Missing Values After KNN Imputation', fontsize=14)\n",
    "sns.heatmap(df_gan.isnull(), cbar=False, cmap='viridis', ax=axes[1])\n",
    "axes[1].set_title('Missing Values (KNN Fallback)', fontsize=14)\n",
    "try:\n",
    "    plt.savefig(os.path.join(PLOT_DIR, 'missing_values_after_imputation.png'))\n",
    "    print(\"Graph 16 saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Graph 16: {e}\")\n",
    "plt.show()\n",
    "\n",
    "# Use KNN-imputed data for main pipeline\n",
    "X = df_knn.drop('Potability', axis=1)\n",
    "y = df_knn['Potability']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Graph 17: Class Distribution After SMOTE (Previously Graph 7 in the snippet)\n",
    "print(\"\\n### Graph 17: Class Distribution After SMOTE ###\")\n",
    "print(\"This bar plot shows the class distribution after applying SMOTE. The classes are now balanced, ensuring that the model won’t be biased toward the majority class (non-potable). Equal counts of potable (1) and non-potable (0) samples confirm SMOTE’s effectiveness in addressing class imbalance.\")\n",
    "\n",
    "# FIX: legend=False is invalid for sns.countplot. Replaced with correct argument.\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=pd.Series(y_train_smote), palette='Set2')\n",
    "plt.title('Class Distribution After SMOTE\\n(0 = Non-Potable, 1 = Potable)', fontsize=14, pad=15)\n",
    "plt.xlabel('Potability', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "try:\n",
    "    plt.savefig(os.path.join(PLOT_DIR, 'class_distribution_smote.png'))\n",
    "    print(\"Graph 17 saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Graph 17: {e}\")\n",
    "plt.show()\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check for NaNs in scaled data\n",
    "print(\"NaN in X_train_scaled:\", np.any(np.isnan(X_train_scaled)))\n",
    "print(\"NaN in X_test_scaled:\", np.any(np.isnan(X_test_scaled)))\n",
    "\n",
    "# Graph 18: Scaled Feature Distribution (Previously Graph 8 in the snippet)\n",
    "print(\"\\n### Graph 18: Scaled Feature Distribution ###\")\n",
    "print(\"These histograms compare the scaled distributions of 'ph' and 'Chloramines*Sulfate' for the training and test sets. Scaling ensures features are on the same scale, which is crucial for models like neural networks and gradient boosting. Similar distributions between train and test sets indicate proper scaling and data consistency.\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.histplot(X_train_scaled[:, 0], kde=True, color='blue', label='Scaled ph (Train)', ax=axes[0])\n",
    "sns.histplot(X_test_scaled[:, 0], kde=True, color='orange', label='Scaled ph (Test)', ax=axes[0])\n",
    "axes[0].set_title('Scaled ph Distribution', fontsize=14)\n",
    "axes[0].set_xlabel('Scaled ph', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "sns.histplot(X_train_scaled[:, -1], kde=True, color='blue', label='Scaled Chloramines*Sulfate (Train)', ax=axes[1])\n",
    "sns.histplot(X_test_scaled[:, -1], kde=True, color='orange', label='Scaled Chloramines*Sulfate (Test)', ax=axes[1])\n",
    "axes[1].set_title('Scaled Chloramines*Sulfate Distribution', fontsize=14)\n",
    "axes[1].set_xlabel('Scaled Chloramines*Sulfate', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle='--', alpha=0.7)\n",
    "try:\n",
    "    plt.savefig(os.path.join(PLOT_DIR, 'scaled_features_distribution.png'))\n",
    "    print(\"Graph 18 saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Graph 18: {e}\")\n",
    "plt.show()\n",
    "\n",
    "# Define model directory\n",
    "MODEL_DIR = r\"C:\\Users\\iasis\\Ultimate_Water_Potability_Prediction\\models\"\n",
    "\n",
    "# Save scaler\n",
    "try:\n",
    "    with open(os.path.join(MODEL_DIR, 'scaler.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(\"Scaler saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving scaler: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b34e0a-05ac-4120-9f7a-8594df7bbd0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define XGBoost\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m xgb \u001b[38;5;241m=\u001b[39m \u001b[43mXGBClassifier\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m grid_search_xgb \u001b[38;5;241m=\u001b[39m GridSearchCV(xgb, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Define XGBoost\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Best model\n",
    "xgb_best = grid_search_xgb.best_estimator_\n",
    "print(\"Best XGBoost Parameters:\", grid_search_xgb.best_params_)\n",
    "\n",
    "# Graph 9: XGBoost CV Scores\n",
    "cv_results_xgb = pd.DataFrame(grid_search_xgb.cv_results_)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cv_results_xgb['mean_test_score'], marker='o')\n",
    "plt.title('XGBoost Cross-Validation F1-Scores During Grid Search')\n",
    "plt.xlabel('Parameter Combination Index')\n",
    "plt.ylabel('Mean F1-Score')\n",
    "plt.savefig('plots/xgb_cv_scores.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "with open('models/xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_best, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02f311a-1d73-4637-8402-4ee190aedc4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LGBMClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define LightGBM\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lgbm \u001b[38;5;241m=\u001b[39m \u001b[43mLGBMClassifier\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m param_grid_lgbm \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m grid_search_lgbm \u001b[38;5;241m=\u001b[39m GridSearchCV(lgbm, param_grid_lgbm, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LGBMClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Define LightGBM\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "grid_search_lgbm = GridSearchCV(lgbm, param_grid_lgbm, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search_lgbm.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Best model\n",
    "lgbm_best = grid_search_lgbm.best_estimator_\n",
    "print(\"Best LightGBM Parameters:\", grid_search_lgbm.best_params_)\n",
    "\n",
    "# Graph 10: LightGBM CV Scores\n",
    "cv_results_lgbm = pd.DataFrame(grid_search_lgbm.cv_results_)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cv_results_lgbm['mean_test_score'], marker='o')\n",
    "plt.title('LightGBM Cross-Validation F1-Scores During Grid Search')\n",
    "plt.xlabel('Parameter Combination Index')\n",
    "plt.ylabel('Mean F1-Score')\n",
    "plt.savefig('plots/lgbm_cv_scores.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "with open('models/lgbm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lgbm_best, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92bb058-733c-4eef-b64c-9b012e26a511",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CatBoostClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define CatBoost\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m catboost \u001b[38;5;241m=\u001b[39m \u001b[43mCatBoostClassifier\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m param_grid_catboost \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m grid_search_catboost \u001b[38;5;241m=\u001b[39m GridSearchCV(catboost, param_grid_catboost, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CatBoostClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Define CatBoost\n",
    "catboost = CatBoostClassifier(random_state=42, verbose=0)\n",
    "param_grid_catboost = {\n",
    "    'iterations': [100, 200],\n",
    "    'depth': [4, 6],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "grid_search_catboost = GridSearchCV(catboost, param_grid_catboost, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search_catboost.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Best model\n",
    "catboost_best = grid_search_catboost.best_estimator_\n",
    "print(\"Best CatBoost Parameters:\", grid_search_catboost.best_params_)\n",
    "\n",
    "# Graph 11: CatBoost CV Scores\n",
    "cv_results_catboost = pd.DataFrame(grid_search_catboost.cv_results_)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cv_results_catboost['mean_test_score'], marker='o')\n",
    "plt.title('CatBoost Cross-Validation F1-Scores During Grid Search')\n",
    "plt.xlabel('Parameter Combination Index')\n",
    "plt.ylabel('Mean F1-Score')\n",
    "plt.savefig('plots/catboost_cv_scores.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "with open('models/catboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(catboost_best, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dcf60f5-94ae-4658-9dee-7133909432e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define neural network\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[0;32m      3\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(X_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[0;32m      4\u001b[0m     Dropout(\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m      5\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      6\u001b[0m     Dropout(\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m      7\u001b[0m     Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      8\u001b[0m     Dropout(\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m      9\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m nn_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Define neural network\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model (reduced epochs for faster execution)\n",
    "history = nn_model.fit(X_train_scaled, y_train_smote, epochs=20, batch_size=32, \n",
    "                      validation_split=0.2, verbose=1)\n",
    "\n",
    "# Graph 12: Neural Network Loss Curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Neural Network Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('plots/nn_loss_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Graph 13: Neural Network Accuracy Curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Neural Network Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('plots/nn_accuracy_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "nn_model.save('models/nn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392a47f-0e46-4f41-96fe-2b687c1b1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=100, random_state=42)),\n",
    "    ('lgbm', LGBMClassifier(n_estimators=100, random_state=42)),\n",
    "    ('catboost', CatBoostClassifier(iterations=100, random_state=42, verbose=0))\n",
    "]\n",
    "\n",
    "# Define meta-learner\n",
    "meta_learner = LogisticRegression()\n",
    "\n",
    "# Define stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "# Train stacking model\n",
    "stacking_clf.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Graph 14: Feature Importance from Random Forest\n",
    "rf_model = stacking_clf.named_estimators_['rf']\n",
    "rf_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "rf_importance.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance from Random Forest (Stacking Base Model)')\n",
    "plt.xlabel('Importance')\n",
    "plt.savefig('plots/rf_feature_importance_stacking.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "with open('models/stacking_model.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ce77b-8dd0-45d8-a5f6-ba9783b30d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TPOT classifier (reduced parameters for faster execution)\n",
    "tpot = TPOTClassifier(generations=3, population_size=10, cv=5, scoring='f1', random_state=42, verbosity=2, n_jobs=-1)\n",
    "\n",
    "# Train TPOT\n",
    "tpot.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Graph 15: TPOT Best Pipeline Score\n",
    "best_pipeline_score = tpot.fitted_pipeline_.score(X_test_scaled, y_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Best TPOT Pipeline'], [best_pipeline_score], color='green')\n",
    "plt.title('Best TPOT Pipeline Test Score (F1)')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.savefig('plots/tpot_best_score.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "tpot.export('models/tpot_pipeline.py')\n",
    "with open('models/tpot_model.pkl', 'wb') as f:\n",
    "    pickle.dump(tpot.fitted_pipeline_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd20ee-eb35-410e-a663-9dcae24664a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return y_pred, y_pred_proba, {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'roc_auc': roc_auc}\n",
    "\n",
    "# Evaluate models\n",
    "xgb_pred, xgb_proba, xgb_metrics = evaluate_model(xgb_best, X_test_scaled, y_test, \"XGBoost\")\n",
    "lgbm_pred, lgbm_proba, lgbm_metrics = evaluate_model(lgbm_best, X_test_scaled, y_test, \"LightGBM\")\n",
    "catboost_pred, catboost_proba, catboost_metrics = evaluate_model(catboost_best, X_test_scaled, y_test, \"CatBoost\")\n",
    "nn_pred = (nn_model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "nn_proba = nn_model.predict(X_test_scaled)\n",
    "nn_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, nn_pred),\n",
    "    'precision': precision_score(y_test, nn_pred),\n",
    "    'recall': recall_score(y_test, nn_pred),\n",
    "    'f1': f1_score(y_test, nn_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, nn_proba)\n",
    "}\n",
    "print(\"\\nNeural Network Performance:\")\n",
    "for metric, value in nn_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "stacking_pred, stacking_proba, stacking_metrics = evaluate_model(stacking_clf, X_test_scaled, y_test, \"Stacking\")\n",
    "tpot_pred, tpot_proba, tpot_metrics = evaluate_model(tpot.fitted_pipeline_, X_test_scaled, y_test, \"TPOT\")\n",
    "\n",
    "# Graph 16: Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "for i, (pred, name) in enumerate([(xgb_pred, 'XGBoost'), (lgbm_pred, 'LightGBM'), (catboost_pred, 'CatBoost'), \n",
    "                                 (nn_pred, 'Neural Network'), (stacking_pred, 'Stacking'), (tpot_pred, 'TPOT')]):\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i//3, i%3])\n",
    "    axes[i//3, i%3].set_title(f'{name} Confusion Matrix')\n",
    "    axes[i//3, i%3].set_xlabel('Predicted')\n",
    "    axes[i//3, i%3].set_ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/confusion_matrices.png')\n",
    "plt.show()\n",
    "\n",
    "# Graph 17: ROC Curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "for proba, name in [(xgb_proba, 'XGBoost'), (lgbm_proba, 'LightGBM'), (catboost_proba, 'CatBoost'), \n",
    "                    (nn_proba, 'Neural Network'), (stacking_proba, 'Stacking'), (tpot_proba, 'TPOT')]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, proba):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for All Models')\n",
    "plt.legend()\n",
    "plt.savefig('plots/roc_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Graph 18: Bar Plot of Model Performance\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'CatBoost', 'Neural Network', 'Stacking', 'TPOT'],\n",
    "    'Accuracy': [xgb_metrics['accuracy'], lgbm_metrics['accuracy'], catboost_metrics['accuracy'], nn_metrics['accuracy'], stacking_metrics['accuracy'], tpot_metrics['accuracy']],\n",
    "    'F1-Score': [xgb_metrics['f1'], lgbm_metrics['f1'], catboost_metrics['f1'], nn_metrics['f1'], stacking_metrics['f1'], tpot_metrics['f1']],\n",
    "    'ROC-AUC': [xgb_metrics['roc_auc'], lgbm_metrics['roc_auc'], catboost_metrics['roc_auc'], nn_metrics['roc_auc'], stacking_metrics['roc_auc'], tpot_metrics['roc_auc']]\n",
    "})\n",
    "fig = px.bar(metrics_df, x='Model', y=['Accuracy', 'F1-Score', 'ROC-AUC'], barmode='group', title='Model Performance Comparison')\n",
    "fig.update_layout(xaxis_title='Model', yaxis_title='Score')\n",
    "fig.write_html('plots/model_performance_bar.html')\n",
    "fig.show()\n",
    "\n",
    "# Compare KNN vs. GAN imputation (KNN only, since GAN is skipped)\n",
    "xgb_gan_metrics = xgb_metrics  # Fallback\n",
    "\n",
    "# Graph 19: Imputation Comparison\n",
    "imputation_metrics = pd.DataFrame({\n",
    "    'Imputation': ['KNN', 'KNN (Fallback)'],\n",
    "    'Accuracy': [xgb_metrics['accuracy'], xgb_gan_metrics['accuracy']],\n",
    "    'F1-Score': [xgb_metrics['f1'], xgb_gan_metrics['f1']],\n",
    "    'ROC-AUC': [xgb_metrics['roc_auc'], xgb_gan_metrics['roc_auc']]\n",
    "})\n",
    "fig = px.bar(imputation_metrics, x='Imputation', y=['Accuracy', 'F1-Score', 'ROC-AUC'], barmode='group', title='KNN vs. KNN (Fallback) Imputation (XGBoost)')\n",
    "fig.update_layout(xaxis_title='Imputation Method', yaxis_title='Score')\n",
    "fig.write_html('plots/imputation_comparison.html')\n",
    "fig.show()\n",
    "\n",
    "# Save predictions\n",
    "predictions = pd.DataFrame({\n",
    "    'Sample_ID': range(len(y_test)),\n",
    "    'True_Potability': y_test,\n",
    "    'XGBoost_Pred': xgb_pred,\n",
    "    'LightGBM_Pred': lgbm_pred,\n",
    "    'CatBoost_Pred': catboost_pred,\n",
    "    'NN_Pred': nn_pred.flatten(),\n",
    "    'Stacking_Pred': stacking_pred,\n",
    "    'TPOT_Pred': tpot_pred,\n",
    "    'XGBoost_Proba': xgb_proba,\n",
    "    'LightGBM_Proba': lgbm_proba,\n",
    "    'CatBoost_Proba': catboost_proba,\n",
    "    'NN_Proba': nn_proba.flatten(),\n",
    "    'Stacking_Proba': stacking_proba,\n",
    "    'TPOT_Proba': tpot_proba\n",
    "})\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de33036-c00e-499f-ada0-906e9d2b995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models already saved\n",
    "print(\"Models saved in 'models/' directory:\")\n",
    "print(\"- xgb_model.pkl\")\n",
    "print(\"- lgbm_model.pkl\")\n",
    "print(\"- catboost_model.pkl\")\n",
    "print(\"- nn_model.h5\")\n",
    "print(\"- stacking_model.pkl\")\n",
    "print(\"- tpot_model.pkl\")\n",
    "print(\"- scaler.pkl\")\n",
    "print(\"Predictions saved in 'predictions.csv'\")\n",
    "print(\"Visualizations saved in 'plots/' directory\")\n",
    "\n",
    "# Graph 20: Feature Importance Across Models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "for i, (model, name) in enumerate([(xgb_best, 'XGBoost'), (lgbm_best, 'LightGBM'), (catboost_best, 'CatBoost')]):\n",
    "    importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    importance.sort_values().plot(kind='barh', ax=axes[i])\n",
    "    axes[i].set_title(f'{name} Feature Importance')\n",
    "    axes[i].set_xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/feature_importance_summary.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f0364-b14e-4370-80fc-afc987ac3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "models = {\n",
    "    'XGBoost': xgb_best,\n",
    "    'LightGBM': lgbm_best,\n",
    "    'CatBoost': catboost_best,\n",
    "    'Stacking': stacking_clf\n",
    "}\n",
    "cv_scores = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train_smote, cv=5, scoring='f1', n_jobs=-1)\n",
    "    cv_scores[name] = scores\n",
    "    print(f\"{name} CV F1-Scores: {scores.mean():.4f} (± {scores.std():.4f})\")\n",
    "\n",
    "# Visualize CV Scores\n",
    "cv_df = pd.DataFrame(cv_scores)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=cv_df, palette='Set2')\n",
    "plt.title('5-Fold Cross-Validation F1-Scores for All Models', fontsize=14, pad=15)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('F1-Score', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig('plots/cv_scores_all_models.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84af17-5a0d-4a3f-b8c3-0092a89f532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Compute SHAP values for the stacking model\n",
    "explainer = shap.KernelExplainer(stacking_clf.predict_proba, X_test_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Summary Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values[1], X_test_scaled, feature_names=X.columns, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance for Stacking Model', fontsize=14, pad=15)\n",
    "plt.savefig('plots/shap_importance_stacking.png')\n",
    "plt.show()\n",
    "\n",
    "# Detailed SHAP Summary Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values[1], X_test_scaled, feature_names=X.columns, show=False)\n",
    "plt.title('SHAP Values Distribution for Stacking Model', fontsize=14, pad=15)\n",
    "plt.savefig('plots/shap_values_stacking.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945939c1-aa01-4814-9761-11d6c807cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = Dash(__name__)\n",
    "\n",
    "# Layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Water Potability Prediction Dashboard\"),\n",
    "    html.H3(\"Explore Data and Model Predictions\"),\n",
    "    \n",
    "    # Dropdown to select feature for distribution\n",
    "    html.Label(\"Select Feature for Distribution:\"),\n",
    "    dcc.Dropdown(\n",
    "        id='feature-dropdown',\n",
    "        options=[\n",
    "            {'label': 'ph', 'value': 'ph'},\n",
    "            {'label': 'Chloramines', 'value': 'Chloramines'},\n",
    "            {'label': 'ph_hardness', 'value': 'ph_hardness'},\n",
    "            {'label': 'chloramines_sulfate', 'value': 'chloramines_sulfate'}\n",
    "        ],\n",
    "        value='ph'\n",
    "    ),\n",
    "    dcc.Graph(id='feature-distribution'),\n",
    "    \n",
    "    # Model Performance\n",
    "    dcc.Graph(id='model-performance', figure=px.bar(metrics_df, x='Model', y=['Accuracy', 'F1-Score', 'ROC-AUC'], \n",
    "                                                    barmode='group', title='Model Performance Comparison'))\n",
    "])\n",
    "\n",
    "# Callback to update feature distribution\n",
    "@app.callback(\n",
    "    Output('feature-distribution', 'figure'),\n",
    "    Input('feature-dropdown', 'value')\n",
    ")\n",
    "def update_distribution(selected_feature):\n",
    "    fig = px.histogram(df, x=selected_feature, color='Potability', nbins=30, opacity=0.7,\n",
    "                       title=f'{selected_feature} Distribution by Potability')\n",
    "    fig.update_layout(\n",
    "        xaxis_title=selected_feature,\n",
    "        yaxis_title='Count',\n",
    "        title_font_size=14,\n",
    "        xaxis_title_font_size=12,\n",
    "        yaxis_title_font_size=12,\n",
    "        legend_title='Potability',\n",
    "        xaxis=dict(showgrid=True),\n",
    "        yaxis=dict(showgrid=True),\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, port=8050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df2d0-b449-48eb-b845-0204c418b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Perform t-tests for each feature\n",
    "p_values = {}\n",
    "for column in X.columns:\n",
    "    potable = df_knn[df_knn['Potability'] == 1][column]\n",
    "    non_potable = df_knn[df_knn['Potability'] == 0][column]\n",
    "    _, p = ttest_ind(potable, non_potable, nan_policy='omit')\n",
    "    p_values[column] = p\n",
    "\n",
    "# Visualize p-values\n",
    "p_values_df = pd.DataFrame.from_dict(p_values, orient='index', columns=['p-value'])\n",
    "p_values_df = p_values_df.sort_values('p-value')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='p-value', y=p_values_df.index, data=p_values_df, palette='coolwarm')\n",
    "plt.axvline(0.05, color='red', linestyle='--', label='Significance Threshold (0.05)')\n",
    "plt.title('p-Values of Features (Potable vs Non-Potable)', fontsize=14, pad=15)\n",
    "plt.xlabel('p-Value', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plt.savefig('plots/p_values_features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e72d94-4904-4ec6-986f-79f36182c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models and scaler\n",
    "with open('models/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "with open('models/stacking_model.pkl', 'rb') as f:\n",
    "    stacking_model = pickle.load(f)\n",
    "\n",
    "# Sample new data (replace with actual new data if available)\n",
    "new_data = pd.DataFrame({\n",
    "    'ph': [7.0],\n",
    "    'Hardness': [200.0],\n",
    "    'Solids': [10000.0],\n",
    "    'Chloramines': [7.0],\n",
    "    'Sulfate': [300.0],\n",
    "    'Conductivity': [400.0],\n",
    "    'Organic_carbon': [10.0],\n",
    "    'Trihalomethanes': [50.0],\n",
    "    'Turbidity': [3.0],\n",
    "    'ph_hardness': [7.0 * 200.0],\n",
    "    'chloramines_sulfate': [7.0 * 300.0]\n",
    "})\n",
    "\n",
    "# Preprocess new data\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Predict with stacking model\n",
    "prediction = stacking_model.predict(new_data_scaled)\n",
    "probability = stacking_model.predict_proba(new_data_scaled)[:, 1]\n",
    "\n",
    "# Display results\n",
    "print(\"New Sample Prediction:\")\n",
    "print(f\"Predicted Potability: {'Potable' if prediction[0] == 1 else 'Non-Potable'}\")\n",
    "print(f\"Probability of Potability: {probability[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b3bb9-86a9-4062-aa73-d5ab7dfccbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f352d62-3a8e-4f98-b883-5b7bde145b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68860f8a-3e15-4209-83d6-f82da85bfcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b8b46-3c1b-423f-a70a-2acc9dbf93e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
